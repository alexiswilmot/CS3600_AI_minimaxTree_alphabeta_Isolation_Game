
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebook.ipynb

import time
from isolation import Board
import numpy

# Credits if any
# 1)
# 2)
# 3)

class OpenMoveEvalFn:
    def score(self, game, my_player=None):
        """Score the current game state
        Evaluation function that outputs a score equal to how many
        moves are open for AI player on the board minus how many moves
        are open for Opponent's player on the board.

        Note:
            If you think of better evaluation function, do it in CustomEvalFn below.

            Args
                game (Board): The board and game state.
                my_player (Player object): This specifies which player you are.

            Returns:
                float: The current state's score. MyMoves-OppMoves.

            """

        # TODO: finish this function!
        return len(Board.get_player_moves(game, my_player)) - len(Board.get_opponent_moves(game, my_player))
        #if player 1 is the one you want to find out
        """if (my_player == game.__player_1__):

            return len(Board.get_player_moves(game, my_player)) - len(Board.get_opponent_moves(game, my_player))

        # if player 2 is the one you want to find out
        if (my_player == game.__player_2__):
            return len(Board.get_opponent_moves(game, my_player)) - len(Board.get_active_moves(game, my_player))

        #raise NotImplementedError"""



######################################################################
########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################
######## IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ############
######################################################################
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######
################ END OF LOCAL TEST CODE SECTION ######################

class CustomPlayer:
    # TODO: finish this class!
    """Player that chooses a move using your evaluation function
    and a minimax algorithm with alpha-beta pruning.
    You must finish and test this player to make sure it properly
    uses minimax and alpha-beta to return a good move."""

    def __init__(self, search_depth=3, eval_fn=OpenMoveEvalFn()):
        """Initializes your player.

        if you find yourself with a superior eval function, update the default
        value of `eval_fn` to `CustomEvalFn()`

        Args:
            search_depth (int): The depth to which your agent will search
            eval_fn (function): Evaluation function used by your agent
        """
        self.eval_fn = eval_fn
        self.search_depth = search_depth


    def move(self, game, time_left):
        """Called to determine one move by your agent

        Note:
            1. Do NOT change the name of this 'move' function. We are going to call
            this function directly.
            2. Call alphabeta instead of minimax once implemented.
        Args:
            game (Board): The board and game state.
            time_left (function): Used to determine time left before timeout

        Returns:
            tuple: (int,int): Your best move
        """
        best_move, utility = alphabeta(self, game, time_left, depth=self.search_depth)
        return best_move

    def utility(self, game, my_turn):
        """You can handle special cases here (e.g. endgame)"""
        return self.eval_fn.score(game, self)

###################################################################
########## DON'T WRITE ANY CODE OUTSIDE THE CLASS! ################
###### IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ###########
###################################################################


def minimax(player, game, time_left, depth, my_turn=True, bestSpot=(None, None), bestVal=-1000):
    """Implementation of the minimax algorithm.
    Args:

        player (CustomPlayer): This is the instantiation of CustomPlayer()
            that represents your agent. It is used to call anything you
            need from the CustomPlayer class (the utility() method, for example,
            or any class variables that belong to CustomPlayer()).
        game (Board): A board and game state.
        time_left (function): Used to determine time left before timeout
        depth: Used to track how deep you are in the search tree
        my_turn (bool): True if you are computing scores during your turn.

    Returns:
        (tuple, int): best_move, val
    """
    #print('mini')
    # while time is allowed to keep going:
        # TODO: finish this function!
    #if len(Board.get_active_moves(game)) < 30:
    #    return alphabeta(player, game, time_left, 4, -1000, 1000, my_turn)

    if time_left() <= 5:
        return (bestSpot, OpenMoveEvalFn.score(game, game, player))
    if depth == 0:
        return (bestSpot, OpenMoveEvalFn.score(game,game, player))
    # max level
    if my_turn == True:
        bestMove = None
        bestVal = -1000
        # for each active move option for the current player
        for option in Board.get_active_moves(game):
            # move is the resultant board, over is boolean, winner is winner
            (move, over, winner) = Board.forecast_move(game, option)
            # if the game is over, return 1 for win and -1 for lose
            if over:
                if winner == player:
                    return (option, 1)
                else:
                    return (option, -1)
            (bestSubtreeMove, subtreeVal) = minimax(player, move, time_left, depth - 1, False, option, bestVal)
            # found the highest score subtree
            if subtreeVal > bestVal:
                bestVal = subtreeVal
                bestMove = option
            if time_left() < 5:
                break
        #print(bestMove)
        #print('max')
        return (bestMove, bestVal)
    # if it's a min level, not my_turn
    else:
        bestMove = None
        bestVal = 1000
        # for each active move option for the current player
        for option in Board.get_active_moves(game):
            # move is the resultant board, over is boolean, winner is the winner if there's a winner
            (move, over, winner) = Board.forecast_move(game, option)
            # if the game is over, if the player wins, then return a 1
            if over:
                if winner == player:
                    return (option, 1)
                else:
                    return (option, -1)
            (bestSubtreeMove, subtreeVal) = minimax(player, move, time_left, depth - 1, True, option, bestVal)
            if subtreeVal < bestVal:
                bestMove = option
                bestVal = subtreeVal
            if time_left() < 5:
                break
        #print(bestMove, bestVal)
        #print('min')
        return (bestMove, bestVal)

######################################################################
########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################
######## IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ############
######################################################################
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######
#tests.beatRandom(CustomPlayer)
#tests.minimaxTest(CustomPlayer, minimax)
################ END OF LOCAL TEST CODE SECTION ######################

def alphabeta(player, game, time_left, depth, alpha=float("-inf"), beta=float("inf"), my_turn=True, bestSpot = (None, None)):
    """Implementation of the alphabeta algorithm.

    Args:
        player (CustomPlayer): This is the instantiation of CustomPlayer()
            that represents your agent. It is used to call anything you need
            from the CustomPlayer class (the utility() method, for example,
            or any class variables that belong to CustomPlayer())
        game (Board): A board and game state.
        time_left (function): Used to determine time left before timeout
        depth: Used to track how deep you are in the search tree
        alpha (float): Alpha value for pruning
        beta (float): Beta value for pruning
        my_turn (bool): True if you are computing scores during your turn.

    Returns:
        (tuple, int): best_move, val
    """

    # TODO: finish this function!
    #if depth is 0 or out of time
    #print(len(Board.get_active_moves(game)))
    #print('alpha')
    #if len(Board.get_active_moves(game)) >= 30:
    #    return minimax(player, game, time_left, 2, my_turn, bestSpot)

    if (depth == 0):
        #print("yay")
        return (None, OpenMoveEvalFn.score(game, game, player))
        #return (None, bestSpot)

    if time_left() < 1:
        #print(depth)
        return (None, OpenMoveEvalFn.score(game, game, player))

    # if max level
    if my_turn == True:
        bestVal = -1000
        bestMove = None
        # option are the coordinates of the player's moves
        for option in Board.get_active_moves(game):
            (newBoard, over, winner) =  Board.forecast_move(game, option)
            # if the move ended the game
            if over:
                if winner == player:
                    return (option, 1)
                else:
                    return (option, -1)
            (subtreeMove, subEval) = alphabeta(player, newBoard, time_left, depth - 1, alpha, beta, False, bestSpot)
            if subEval > bestVal:
                bestVal = subEval
                bestMove = option
            alpha = max(alpha, bestVal)
            #stop looking at it if beta <= alpha
            if bestVal >= beta:
                #return (bestMove, bestVal)
                break
            if time_left() <= 1:
                #break
                #print(depth)
                return (bestMove, bestVal)
        return (bestMove, bestVal)
    # min level

    else:
        bestVal = 1000
        bestMove = None
        for option in Board.get_active_moves(game):
            (newBoard, over, winner) = Board.forecast_move(game, option)
            if over:
                if winner == player:
                    return (option, 1)
                else:
                    return (option, -1)
            #moveNum += 1
            (subtreeMove, subEval) = alphabeta(player, newBoard, time_left, depth - 1, alpha, beta, True, bestSpot)
            if subEval < bestVal:
                bestVal = subEval
                bestMove = option
            beta = min(beta, bestVal)
            if bestVal <= alpha:
                return (bestMove, bestVal)
                #break
            if time_left() <= 1:
                #print(depth)
                #break
                return (bestMove, bestVal)
        return (option, bestVal)





    #raise NotImplementedError


######################################################################
########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################
######## IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ############
######################################################################
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######
# tests.name_of_the_test #you can uncomment this line to run your test
#tests.beatRandom(CustomPlayer)


#ig = InteractiveGame(CustomPlayer(), show_legal_moves=True)



#tests.minimaxTest(CustomPlayer, alphabeta)
################ END OF LOCAL TEST CODE SECTION ######################

class CustomEvalFn:
    def __init__(self):
        pass

    def score(self, game, my_player=None):
        """Score the current game state.

        Custom evaluation function that acts however you think it should. This
        is not required but highly encouraged if you want to build the best
        AI possible.

        Args:
            game (Board): The board and game state.
            my_player (Player object): This specifies which player you are.

        Returns:
            float: The current state's score, based on your own heuristic.
        """

        # TODO: finish this function!
        raise NotImplementedError

######################################################################
############ DON'T WRITE ANY CODE OUTSIDE THE CLASS! #################
######## IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ############
######################################################################